"""
çˆ¬è™«åŸºç±»
æä¾›é€šç”¨çš„çˆ¬è™«åŠŸèƒ½å’Œå·¥å…·æ–¹æ³•
"""

import requests
import time
import random
from typing import Optional, Dict, List
from bs4 import BeautifulSoup
import logging
from urllib.parse import urljoin

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


class BaseCrawler:
    """çˆ¬è™«åŸºç±»"""
    
    # User-Agentåˆ—è¡¨ï¼Œç”¨äºè½®æ¢
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    ]
    
    def __init__(self, base_url: str, delay: float = 2.0):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            base_url: åŸºç¡€URL
            delay: è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œé¿å…è¢«å°
        """
        self.base_url = base_url
        self.delay = delay
        self.session = requests.Session()
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # è®¾ç½®é»˜è®¤è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': random.choice(self.USER_AGENTS),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        })
    
    def get(self, url: str, params: Dict = None, max_retries: int = 3) -> Optional[requests.Response]:
        """
        å‘é€GETè¯·æ±‚ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        
        Args:
            url: è¯·æ±‚URL
            params: URLå‚æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            å“åº”å¯¹è±¡ï¼Œå¤±è´¥è¿”å›None
        """
        # æ„å»ºå®Œæ•´URL
        if not url.startswith('http'):
            url = urljoin(self.base_url, url)
        
        for attempt in range(max_retries):
            try:
                # éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
                time.sleep(self.delay + random.uniform(0, 1))
                
                # è½®æ¢User-Agent
                self.session.headers['User-Agent'] = random.choice(self.USER_AGENTS)
                
                response = self.session.get(url, params=params, timeout=30)
                response.raise_for_status()
                
                self.logger.debug(f"âœ… æˆåŠŸè·å–: {url}")
                return response
                
            except requests.exceptions.RequestException as e:
                self.logger.warning(f"âš ï¸ è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {url} - {e}")
                
                if attempt == max_retries - 1:
                    self.logger.error(f"âŒ è¯·æ±‚æœ€ç»ˆå¤±è´¥: {url}")
                    return None
                
                # æŒ‡æ•°é€€é¿
                time.sleep(2 ** attempt)
        
        return None
    
    def parse_html(self, html: str) -> Optional[BeautifulSoup]:
        """
        è§£æHTML
        
        Args:
            html: HTMLå­—ç¬¦ä¸²
            
        Returns:
            BeautifulSoupå¯¹è±¡
        """
        try:
            return BeautifulSoup(html, 'lxml')
        except Exception as e:
            self.logger.error(f"âŒ HTMLè§£æå¤±è´¥: {e}")
            return None
    
    def extract_text(self, element, default: str = "") -> str:
        """
        å®‰å…¨åœ°æå–å…ƒç´ æ–‡æœ¬
        
        Args:
            element: BeautifulSoupå…ƒç´ 
            default: é»˜è®¤å€¼
            
        Returns:
            æ–‡æœ¬å†…å®¹
        """
        if element:
            text = element.get_text(strip=True)
            return text if text else default
        return default
    
    def clean_text(self, text: str) -> str:
        """
        æ¸…ç†æ–‡æœ¬ï¼ˆå»é™¤å¤šä½™ç©ºç™½å­—ç¬¦ç­‰ï¼‰
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        # æ›¿æ¢å¤šä¸ªç©ºç™½å­—ç¬¦ä¸ºå•ä¸ªç©ºæ ¼
        import re
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        return text
    
    def save_html(self, html: str, filename: str):
        """
        ä¿å­˜HTMLåˆ°æ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        
        Args:
            html: HTMLå†…å®¹
            filename: æ–‡ä»¶å
        """
        import os
        os.makedirs('logs/html', exist_ok=True)
        
        filepath = f'logs/html/{filename}'
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(html)
        
        self.logger.info(f"ğŸ’¾ HTMLå·²ä¿å­˜: {filepath}")
    
    def extract_date(self, date_str: str) -> Optional[str]:
        """
        æå–å¹¶æ ‡å‡†åŒ–æ—¥æœŸ
        
        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆå„ç§æ ¼å¼ï¼‰
            
        Returns:
            æ ‡å‡†åŒ–æ—¥æœŸ (YYYY-MM-DD)
        """
        import re
        from datetime import datetime
        
        if not date_str:
            return None
        
        # å°è¯•å„ç§æ—¥æœŸæ ¼å¼
        patterns = [
            (r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', '%Y-%m-%d'),
            (r'(\d{4})-(\d{1,2})-(\d{1,2})', '%Y-%m-%d'),
            (r'(\d{4})\.(\d{1,2})\.(\d{1,2})', '%Y-%m-%d'),
            (r'(\d{4})/(\d{1,2})/(\d{1,2})', '%Y-%m-%d'),
        ]
        
        for pattern, fmt in patterns:
            match = re.search(pattern, date_str)
            if match:
                groups = match.groups()
                if len(groups) == 3:
                    year, month, day = groups
                    try:
                        date_obj = datetime(int(year), int(month), int(day))
                        return date_obj.strftime('%Y-%m-%d')
                    except ValueError:
                        continue
        
        return None
    
    def crawl(self):
        """
        çˆ¬å–æ–¹æ³•ï¼ˆå­ç±»å®ç°ï¼‰
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç°crawlæ–¹æ³•")


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # æµ‹è¯•åŸºç±»åŠŸèƒ½
    crawler = BaseCrawler("https://flk.npc.gov.cn")
    
    # æµ‹è¯•æ—¥æœŸæå–
    test_dates = [
        "2023å¹´12æœˆ31æ—¥",
        "2023-12-31",
        "2023.12.31",
        "å‘å¸ƒäº2023å¹´1æœˆ1æ—¥ç”Ÿæ•ˆ"
    ]
    
    print("ğŸ“… æ—¥æœŸæå–æµ‹è¯•:")
    for date_str in test_dates:
        result = crawler.extract_date(date_str)
        print(f"  {date_str} -> {result}")
