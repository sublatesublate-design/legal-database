"""
å…¨å›½äººå¤§æ³•å¾‹æ³•è§„æ•°æ®åº“çˆ¬è™«
ä» flk.npc.gov.cn é‡‡é›†æ³•å¾‹æ•°æ®
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from crawler.base_crawler import BaseCrawler
from database.db_manager import DatabaseManager
import re
from typing import List, Dict, Optional
from tqdm import tqdm


class NPCCrawler(BaseCrawler):
    """å…¨å›½äººå¤§æ³•è§„åº“çˆ¬è™«"""
    
    # æ³•å¾‹åˆ†ç±»æ˜ å°„
    CATEGORIES = {
        'law': 'æ³•å¾‹',
        'admin_reg': 'è¡Œæ”¿æ³•è§„',
        'judicial_interpretation': 'å¸æ³•è§£é‡Š',
    }
    
    def __init__(self, db_manager: DatabaseManager):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            db_manager: æ•°æ®åº“ç®¡ç†å™¨å®ä¾‹
        """
        super().__init__(base_url="https://flk.npc.gov.cn", delay=3.0)
        self.db = db_manager
    
    def crawl_law_list(self, category: str = 'law', limit: int = None) -> List[Dict]:
        """
        çˆ¬å–æ³•å¾‹åˆ—è¡¨
        
        Args:
            category: æ³•å¾‹ç±»åˆ«
            limit: é™åˆ¶æ•°é‡ï¼ˆæµ‹è¯•ç”¨ï¼‰
            
        Returns:
            æ³•å¾‹åŸºæœ¬ä¿¡æ¯åˆ—è¡¨
        """
        self.logger.info(f"ğŸ” å¼€å§‹çˆ¬å–æ³•å¾‹åˆ—è¡¨: {self.CATEGORIES.get(category, category)}")
        
        laws = []
        page = 1
        
        while True:
            # æ„å»ºåˆ—è¡¨é¡µURLï¼ˆéœ€è¦æ ¹æ®å®é™…ç½‘ç«™è°ƒæ•´ï¼‰
            url = f"/fl.html"  # æ³•å¾‹é¦–é¡µ
            
            response = self.get(url)
            if not response:
                break
            
            soup = self.parse_html(response.text)
            if not soup:
                break
            
            # è§£ææ³•å¾‹åˆ—è¡¨ï¼ˆéœ€è¦æ ¹æ®å®é™…HTMLç»“æ„è°ƒæ•´ï¼‰
            law_items = soup.select('.law-item')  # ç¤ºä¾‹é€‰æ‹©å™¨
            
            if not law_items:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°æ³•å¾‹æ¡ç›®ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´é€‰æ‹©å™¨")
                break
            
            for item in law_items:
                try:
                    law_info = self._parse_law_item(item)
                    if law_info:
                        laws.append(law_info)
                        
                        if limit and len(laws) >= limit:
                            return laws
                except Exception as e:
                    self.logger.error(f"âŒ è§£ææ³•å¾‹æ¡ç›®å¤±è´¥: {e}")
                    continue
            
            page += 1
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
            if not soup.select('.next-page'):
                break
        
        self.logger.info(f"âœ… å…±æ‰¾åˆ° {len(laws)} éƒ¨æ³•å¾‹")
        return laws
    
    def _parse_law_item(self, item) -> Optional[Dict]:
        """
        è§£æå•ä¸ªæ³•å¾‹æ¡ç›®
        
        Args:
            item: BeautifulSoupå…ƒç´ 
            
        Returns:
            æ³•å¾‹åŸºæœ¬ä¿¡æ¯å­—å…¸
        """
        try:
            # æå–æ ‡é¢˜å’Œé“¾æ¥ï¼ˆéœ€è¦æ ¹æ®å®é™…HTMLè°ƒæ•´ï¼‰
            title_elem = item.select_one('.title a')
            if not title_elem:
                return None
            
            title = self.clean_text(title_elem.get_text())
            detail_url = title_elem.get('href', '')
            
            # æå–å…¶ä»–ä¿¡æ¯
            info = {
                'title': title,
                'detail_url': detail_url,
                'category': 'æ³•å¾‹',
                'status': 'active'
            }
            
            # æå–å‘å¸ƒæ—¥æœŸ
            date_elem = item.select_one('.publish-date')
            if date_elem:
                date_str = self.extract_text(date_elem)
                info['publish_date'] = self.extract_date(date_str)
            
            # æå–æ–‡å·
            doc_num_elem = item.select_one('.doc-number')
            if doc_num_elem:
                info['document_number'] = self.extract_text(doc_num_elem)
            
            return info
            
        except Exception as e:
            self.logger.error(f"âŒ è§£ææ¡ç›®å¤±è´¥: {e}")
            return None
    
    def crawl_law_detail(self, law_info: Dict) -> Optional[Dict]:
        """
        çˆ¬å–æ³•å¾‹è¯¦æƒ…ï¼ˆå…¨æ–‡å’Œæ³•æ¡ï¼‰
        
        Args:
            law_info: æ³•å¾‹åŸºæœ¬ä¿¡æ¯
            
        Returns:
            å®Œæ•´çš„æ³•å¾‹æ•°æ®
        """
        url = law_info.get('detail_url')
        if not url:
            return None
        
        self.logger.info(f"ğŸ“– çˆ¬å–æ³•å¾‹è¯¦æƒ…: {law_info['title']}")
        
        response = self.get(url)
        if not response:
            return None
        
        soup = self.parse_html(response.text)
        if not soup:
            return None
        
        # æå–å…¨æ–‡
        content_elem = soup.select_one('.law-content')
        if content_elem:
            full_text = content_elem.get_text(separator='\n', strip=True)
            law_info['full_text'] = full_text
            law_info['source_url'] = url
            
            # æå–æ³•æ¡
            articles = self._extract_articles(content_elem)
            law_info['articles'] = articles
        
        return law_info
    
    def _extract_articles(self, content_elem) -> List[Dict]:
        """
        ä»å…¨æ–‡ä¸­æå–æ³•æ¡
        
        Args:
            content_elem: å†…å®¹å…ƒç´ 
            
        Returns:
            æ³•æ¡åˆ—è¡¨
        """
        articles = []
        
        # æŸ¥æ‰¾æ‰€æœ‰æ¡æ–‡ï¼ˆé€šå¸¸ä»¥"ç¬¬Xæ¡"å¼€å¤´ï¼‰
        text = content_elem.get_text()
        pattern = r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ\d]+)æ¡\s+(.*?)(?=ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ\d]+æ¡|$)'
        
        matches = re.finditer(pattern, text, re.DOTALL)
        
        for idx, match in enumerate(matches, 1):
            article_num_cn = match.group(1)
            article_content = match.group(2).strip()
            
            articles.append({
                'article_number': f'ç¬¬{article_num_cn}æ¡',
                'article_index': idx,
                'content': article_content
            })
        
        self.logger.debug(f"  æå–äº† {len(articles)} æ¡æ³•æ¡")
        return articles
    
    def save_law(self, law_data: Dict) -> bool:
        """
        ä¿å­˜æ³•å¾‹åˆ°æ•°æ®åº“
        
        Args:
            law_data: æ³•å¾‹æ•°æ®
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = self.db.get_law_by_title(law_data['title'])
            if existing:
                self.logger.info(f"â­ï¸  æ³•å¾‹å·²å­˜åœ¨: {law_data['title']}")
                return False
            
            # æ’å…¥æ³•å¾‹ä¸»è®°å½•
            law_id = self.db.insert_law(law_data)
            self.logger.info(f"âœ… ä¿å­˜æ³•å¾‹: {law_data['title']} (ID: {law_id})")
            
            # æ’å…¥æ³•æ¡
            if 'articles' in law_data:
                for article in law_data['articles']:
                    article['law_id'] = law_id
                    self.db.insert_article(article)
                
                self.logger.debug(f"  ä¿å­˜äº† {len(law_data['articles'])} æ¡æ³•æ¡")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æ³•å¾‹å¤±è´¥: {e}")
            return False
    
    def crawl(self, limit: int = None):
        """
        æ‰§è¡Œå®Œæ•´çˆ¬å–æµç¨‹
        
        Args:
            limit: é™åˆ¶çˆ¬å–æ•°é‡
        """
        self.logger.info("ğŸš€ å¼€å§‹çˆ¬å–å…¨å›½äººå¤§æ³•å¾‹æ•°æ®åº“")
        
        # 1. çˆ¬å–æ³•å¾‹åˆ—è¡¨
        law_list = self.crawl_law_list(limit=limit)
        
        # 2. çˆ¬å–æ¯éƒ¨æ³•å¾‹çš„è¯¦æƒ…
        success_count = 0
        
        for law_info in tqdm(law_list, desc="çˆ¬å–æ³•å¾‹è¯¦æƒ…"):
            # è·å–è¯¦æƒ…
            law_data = self.crawl_law_detail(law_info)
            
            if law_data:
                # ä¿å­˜åˆ°æ•°æ®åº“
                if self.save_law(law_data):
                    success_count += 1
        
        self.logger.info(f"âœ… çˆ¬å–å®Œæˆï¼æˆåŠŸä¿å­˜ {success_count}/{len(law_list)} éƒ¨æ³•å¾‹")
        
        # æ˜¾ç¤ºç»Ÿè®¡
        stats = self.db.get_statistics()
        self.logger.info(f"ğŸ“Š æ•°æ®åº“ç»Ÿè®¡: {stats}")


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # åˆ›å»ºæ•°æ®åº“
    db = DatabaseManager()
    
    # åˆ›å»ºçˆ¬è™«
    crawler = NPCCrawler(db)
    
    # æµ‹è¯•æ¨¡å¼ï¼šåªçˆ¬å–5éƒ¨æ³•å¾‹
    crawler.crawl(limit=5)
